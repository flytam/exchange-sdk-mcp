# Bitget API æ–‡æ¡£çˆ¬è™«

åŸºäº crawl4ai 0.7.x æ¡†æ¶å®ç°çš„ Bitget API æ–‡æ¡£çˆ¬å–å·¥å…·ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸš€ åŸºäº crawl4ai 0.7.x æ¡†æ¶ï¼Œæ”¯æŒå¼‚æ­¥çˆ¬å–
- ğŸ“„ è‡ªåŠ¨æå–é¡µé¢æ ‡é¢˜ã€æè¿°ã€HTTPè·¯å¾„å’Œå†…å®¹
- ğŸ”„ æ™ºèƒ½åˆ†é¡µå¤„ç†ï¼Œè‡ªåŠ¨è·Ÿè¸ª `next_page` é“¾æ¥
- ğŸ“ HTML å†…å®¹è‡ªåŠ¨è½¬æ¢ä¸º Markdown æ ¼å¼
- ğŸ’¾ ç»“æœä¿å­˜ä¸ºåºåˆ—åŒ– JSON æ–‡ä»¶
- ğŸ“Š è¯¦ç»†çš„çˆ¬å–æ—¥å¿—å’Œè¿›åº¦æŠ¥å‘Š
- âš™ï¸ å¯é…ç½®çš„çˆ¬å–å‚æ•°
- ğŸ›¡ï¸ å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

## å®‰è£…ä¾èµ–

```bash
# å®‰è£… Python ä¾èµ–
pip install -r requirements.txt

# æˆ–è€…æ‰‹åŠ¨å®‰è£…
pip install crawl4ai>=0.7.0 markdownify>=0.11.6
```

## å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ç”¨æ³•

```bash
# ä½¿ç”¨é»˜è®¤è®¾ç½®è¿è¡Œ
python run.py

# æˆ–è€…ç›´æ¥è¿è¡Œçˆ¬è™«è„šæœ¬
python crawler.py
```

### é«˜çº§ç”¨æ³•

```bash
# é™åˆ¶æœ€å¤§é¡µé¢æ•°
python run.py --max-pages 20

# æŒ‡å®šè¾“å‡ºæ–‡ä»¶å
python run.py --output my_bitget_docs.json

# è®¾ç½®è¯·æ±‚é—´éš”
python run.py --delay 2.0

# è¯•è¿è¡Œæ¨¡å¼ï¼ˆåªçˆ¬å–ç¬¬ä¸€é¡µï¼‰
python run.py --dry-run

# æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
python run.py --verbose

# è‡ªå®šä¹‰è¾“å‡ºç›®å½•
python run.py --output-dir ./my_output
```

## é…ç½®è¯´æ˜

### Schema é…ç½® (schema.json)

```json
{
  "name": "www.bitget.com Schema",
  "baseSelector": ".container",
  "fields": [
    {
      "name": "title",
      "selector": "div.theme-doc-markdown.markdown > h1",
      "type": "text"
    },
    {
      "name": "description",
      "selector": "div.theme-doc-markdown.markdown > p",
      "type": "text"
    },
    {
      "name": "http_path",
      "selector": "div.theme-doc-markdown.markdown > ul > li:nth-of-type(1)",
      "type": "text"
    },
    {
      "name": "next_page",
      "selector": "div > nav.pagination-nav.docusaurus-mt-lg > a.pagination-nav__link.pagination-nav__link--next:nth-of-type(2)",
      "type": "attribute",
      "attribute": "href"
    },
    {
      "name": "content",
      "selector": "div.theme-doc-markdown.markdown",
      "type": "text"
    }
  ]
}
```

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# æœ€å¤§çˆ¬å–é¡µé¢æ•°
export BITGET_MAX_PAGES=100

# é¡µé¢è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
export BITGET_PAGE_TIMEOUT=30000

# è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
export BITGET_DELAY=1.0

# é¡µé¢åŠ è½½ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
export BITGET_PAGE_DELAY=2.0
```

## è¾“å‡ºæ ¼å¼

çˆ¬å–ç»“æœä¿å­˜ä¸º JSON æ ¼å¼ï¼Œæ¯ä¸ªé¡µé¢åŒ…å«ä»¥ä¸‹å­—æ®µï¼š

```json
[
  {
    "title": "é¡µé¢æ ‡é¢˜",
    "description": "é¡µé¢æè¿°",
    "http_path": "HTTP è·¯å¾„ä¿¡æ¯",
    "next_page": "ä¸‹ä¸€é¡µé“¾æ¥",
    "content": "é¡µé¢å†…å®¹ï¼ˆMarkdown æ ¼å¼ï¼‰",
    "current_url": "å½“å‰é¡µé¢ URL",
    "crawled_at": "çˆ¬å–æ—¶é—´æˆ³"
  }
]
```

## æ–‡ä»¶ç»“æ„

```
bitget/
â”œâ”€â”€ crawler.py          # ä¸»çˆ¬è™«è„šæœ¬
â”œâ”€â”€ run.py             # è¿è¡Œè„šæœ¬
â”œâ”€â”€ config.py          # é…ç½®æ–‡ä»¶
â”œâ”€â”€ schema.json        # æ•°æ®æå– Schema
â”œâ”€â”€ requirements.txt   # Python ä¾èµ–
â”œâ”€â”€ README.md         # é¡¹ç›®æ–‡æ¡£
â””â”€â”€ output/           # è¾“å‡ºç›®å½•
    â”œâ”€â”€ bitget_docs_*.json  # çˆ¬å–ç»“æœ
    â””â”€â”€ crawler_*.log       # çˆ¬å–æ—¥å¿—
```

## ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºæœ¬çˆ¬å–

```python
import asyncio
from crawler import BitgetCrawler

async def main():
    crawler = BitgetCrawler()
    results = await crawler.crawl_all_pages(max_pages=10)
    output_file = crawler.save_results()
    print(f"ç»“æœä¿å­˜åœ¨: {output_file}")

asyncio.run(main())
```

### 2. è‡ªå®šä¹‰é…ç½®

```python
import asyncio
from crawler import BitgetCrawler

async def main():
    crawler = BitgetCrawler(
        schema_path="custom_schema.json",
        output_dir="custom_output"
    )

    results = await crawler.crawl_all_pages(max_pages=50)

    if results:
        output_file = crawler.save_results("custom_output.json")
        crawler.print_summary()

asyncio.run(main())
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **ä¾èµ–å®‰è£…å¤±è´¥**

   ```bash
   # å‡çº§ pip
   pip install --upgrade pip

   # ä½¿ç”¨å›½å†…é•œåƒ
   pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple/
   ```

2. **çˆ¬å–å¤±è´¥**

   - æ£€æŸ¥ç½‘ç»œè¿æ¥
   - ç¡®è®¤ç›®æ ‡ç½‘ç«™å¯è®¿é—®
   - å¢åŠ é¡µé¢è¶…æ—¶æ—¶é—´
   - æ£€æŸ¥ schema é€‰æ‹©å™¨æ˜¯å¦æ­£ç¡®

3. **å†…å­˜ä¸è¶³**
   - å‡å°‘ `max_pages` å‚æ•°
   - å¢åŠ è¯·æ±‚é—´éš” `--delay`
   - åˆ†æ‰¹æ¬¡çˆ¬å–

### è°ƒè¯•æ¨¡å¼

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
python run.py --verbose

# è¯•è¿è¡Œæ¨¡å¼
python run.py --dry-run

# æ£€æŸ¥ç¬¬ä¸€é¡µæå–ç»“æœ
python run.py --max-pages 1 --verbose
```

## æŠ€æœ¯ç‰¹æ€§

- **å¼‚æ­¥çˆ¬å–**: åŸºäº asyncio å’Œ crawl4ai çš„å¼‚æ­¥æ¶æ„
- **æ™ºèƒ½é‡è¯•**: è‡ªåŠ¨å¤„ç†ç½‘ç»œé”™è¯¯å’Œè¶…æ—¶
- **å†…å®¹æ¸…ç†**: è‡ªåŠ¨æ¸…ç† HTML æ ‡ç­¾å’Œå¤šä½™ç©ºæ ¼
- **URL æ ‡å‡†åŒ–**: è‡ªåŠ¨å¤„ç†ç›¸å¯¹é“¾æ¥å’Œç»å¯¹é“¾æ¥
- **å¾ªç¯æ£€æµ‹**: é˜²æ­¢æ— é™å¾ªç¯çˆ¬å–
- **è¿›åº¦ç›‘æ§**: å®æ—¶æ˜¾ç¤ºçˆ¬å–è¿›åº¦å’Œç»Ÿè®¡ä¿¡æ¯

## è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäº MIT è®¸å¯è¯å¼€æºã€‚

## è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼
