#!/usr/bin/env python3
"""
Bitget çˆ¬è™«è¿è¡Œè„šæœ¬
æä¾›å‘½ä»¤è¡Œæ¥å£æ¥æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
"""

import argparse
import asyncio
import sys
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from crawler import BitgetCrawler
from config import *


def check_dependencies():
    """æ£€æŸ¥ä¾èµ–æ˜¯å¦å®‰è£…"""
    missing_deps = []
    
    try:
        import crawl4ai
    except ImportError:
        missing_deps.append("crawl4ai>=0.7.0")
        
    try:
        import markdownify
    except ImportError:
        missing_deps.append("markdownify>=0.11.6")
        
    if missing_deps:
        print("âŒ ç¼ºå°‘ä»¥ä¸‹ä¾èµ–:")
        for dep in missing_deps:
            print(f"   - {dep}")
        print("\nè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ä¾èµ–:")
        print(f"pip install -r {REQUIREMENTS_FILE}")
        return False
        
    return True


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="Bitget API æ–‡æ¡£çˆ¬è™«",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python run.py                    # ä½¿ç”¨é»˜è®¤è®¾ç½®è¿è¡Œ
  python run.py --max-pages 20     # é™åˆ¶æœ€å¤§é¡µé¢æ•°
  python run.py --output custom.json  # æŒ‡å®šè¾“å‡ºæ–‡ä»¶å
  python run.py --verbose          # æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
        """
    )
    
    parser.add_argument(
        "--max-pages",
        type=int,
        default=MAX_PAGES,
        help=f"æœ€å¤§çˆ¬å–é¡µé¢æ•° (é»˜è®¤: {MAX_PAGES})"
    )
    
    parser.add_argument(
        "--output",
        type=str,
        help="è¾“å‡ºæ–‡ä»¶å (é»˜è®¤: è‡ªåŠ¨ç”Ÿæˆæ—¶é—´æˆ³æ–‡ä»¶å)"
    )
    
    parser.add_argument(
        "--schema",
        type=str,
        default=str(SCHEMA_FILE),
        help=f"Schema æ–‡ä»¶è·¯å¾„ (é»˜è®¤: {SCHEMA_FILE})"
    )
    
    parser.add_argument(
        "--output-dir",
        type=str,
        default=str(OUTPUT_DIR),
        help=f"è¾“å‡ºç›®å½• (é»˜è®¤: {OUTPUT_DIR})"
    )
    
    parser.add_argument(
        "--delay",
        type=float,
        default=DELAY_BETWEEN_REQUESTS,
        help=f"è¯·æ±‚é—´éš”ç§’æ•° (é»˜è®¤: {DELAY_BETWEEN_REQUESTS})"
    )
    
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—"
    )
    
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="è¯•è¿è¡Œæ¨¡å¼ï¼Œåªçˆ¬å–ç¬¬ä¸€é¡µ"
    )
    
    return parser.parse_args()


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Bitget API æ–‡æ¡£çˆ¬è™«")
    print("=" * 50)
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        sys.exit(1)
        
    # æ£€æŸ¥ schema æ–‡ä»¶
    if not Path(args.schema).exists():
        print(f"âŒ Schema æ–‡ä»¶ä¸å­˜åœ¨: {args.schema}")
        sys.exit(1)
        
    print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"   Schema æ–‡ä»¶: {args.schema}")
    print(f"   è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"   æœ€å¤§é¡µé¢æ•°: {args.max_pages if not args.dry_run else 1}")
    print(f"   è¯·æ±‚é—´éš”: {args.delay}ç§’")
    print(f"   è¯¦ç»†æ—¥å¿—: {'æ˜¯' if args.verbose else 'å¦'}")
    print(f"   è¯•è¿è¡Œæ¨¡å¼: {'æ˜¯' if args.dry_run else 'å¦'}")
    print()
    
    try:
        # åˆå§‹åŒ–çˆ¬è™«
        crawler = BitgetCrawler(
            schema_path=args.schema,
            output_dir=args.output_dir
        )
        
        # è®¾ç½®å»¶è¿Ÿ
        if hasattr(crawler, 'delay'):
            crawler.delay = args.delay
            
        # æ‰§è¡Œçˆ¬å–
        max_pages = 1 if args.dry_run else args.max_pages
        print(f"ğŸ”„ å¼€å§‹çˆ¬å– (æœ€å¤§ {max_pages} é¡µ)...")
        
        results = await crawler.crawl_all_pages(max_pages=max_pages)
        
        # ä¿å­˜ç»“æœ
        if results:
            output_file = crawler.save_results(args.output)
            crawler.print_summary()
            
            if args.dry_run:
                print(f"\nâœ… è¯•è¿è¡Œå®Œæˆï¼")
                print(f"   çˆ¬å–äº† {len(results)} é¡µ")
                print(f"   ç»“æœä¿å­˜åœ¨: {output_file}")
                print(f"   å¦‚æœç»“æœæ­£ç¡®ï¼Œè¯·ç§»é™¤ --dry-run å‚æ•°è¿è¡Œå®Œæ•´çˆ¬å–")
            else:
                print(f"\nâœ… çˆ¬å–å®Œæˆï¼")
                print(f"   æ€»å…±çˆ¬å–: {len(results)} é¡µ")
                print(f"   ç»“æœä¿å­˜åœ¨: {output_file}")
        else:
            print("âŒ æ²¡æœ‰çˆ¬å–åˆ°ä»»ä½•æ•°æ®")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
        if hasattr(crawler, 'results') and crawler.results:
            output_file = crawler.save_results("bitget_docs_interrupted.json")
            print(f"å·²ä¿å­˜éƒ¨åˆ†ç»“æœ: {output_file}")
    except Exception as e:
        print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        if hasattr(crawler, 'results') and crawler.results:
            output_file = crawler.save_results("bitget_docs_error.json")
            print(f"å·²ä¿å­˜éƒ¨åˆ†ç»“æœ: {output_file}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())